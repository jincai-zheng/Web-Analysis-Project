---
title: "Final"
author: "ZHENG JINCAI"
date: "2019年6月4日"
output: word_document
---

```{r}
#########data preprocessing############3
library(plyr)
library(foreign)
df=read.csv('BEPS.csv')
attach(df)
df=df[,-1]
str(df)
head(df)

##Change data type
df$economic.cond.national=factor(df$economic.cond.national, order=F,levels = c(1,2,3,4,5))
df$economic.cond.household=factor(df$economic.cond.household,order=F, levels = c(1,2,3,4,5))
df$Blair=factor(df$Blair, order=F,levels = c(1,2,3,4,5))
df$Hague=factor(df$Hague, order=F, levels = c(1,2,3,4,5))
df$Kennedy=factor(df$Kennedy, order=F,levels = c(1,2,3,4,5))
df$Europe=factor(df$Europe, order=F, levels = c(1,2,3,4,5,6,7,8,9,10,11))
df$political.knowledge=factor(df$political.knowledge, order=F,levels = c(0,1,2,3))
df$gender=ifelse(gender=="male",1,2)
df$gender=factor(df$gender)
levels(df$vote)[levels(df$vote)%in% c("Labour","Liberal Democrat")]<-"2"
levels(df$vote)[levels(df$vote)%in% c("Conservative")]<-"1"
dim(df)

#check missing value
sum(is.na(df))#
```
```{r}
#Data visualization:
library(psych)
library(ggplot2)
str(df)
summary(df)
describe(df)

##pairplot
pairs(~.,data=df)

#histplot of age by political knowledge
ggplot(df, aes(x=age,fill=gender))+
  geom_histogram(position = "identity", alpha = 0.4)
```
```{r}
#Train Test Split
# Set random seed
set.seed(1)
# 70% observations work as training data
subset = sample(1:nrow(df), 1068) 
df_train = df[subset,]
# The rest 30% are test data 
df_test = df[-subset,]

###Upsmaple
set.seed(1)
train_majority = df_train[which(df_train$vote==2),]
train_minority = df_train[which(df_train$vote==1),]
subset2= sample(1:nrow(train_minority),nrow(train_majority),replace = TRUE)
train_minority_upsampled = train_minority[subset2,] # reproducible results
train_upsampled = rbind(train_majority, train_minority_upsampled)
dim(train_upsampled )
table(train_upsampled$vote) #check whether we get balanced class label
```
```{r}
#############Logistics
set.seed(1)
colnames(df)
glm.fit = glm(vote~.,data = df_train,family = binomial)
summary(glm.fit)

#select significant varibles:
df_train$Blair4=(df_train$Blair=="4")
df_train$Blair5=(df_train$Blair=="5")
df_train$Hague2=(df_train$Hague=="2")
df_train$Hague4=(df_train$Hague=="4")
df_train$Hague5=(df_train$Hague=="5")
df_train$Kennedy4=(df_train$Kennedy=="4")
df_train$Kennedy5=(df_train$Kennedy=="5")
df_train$Europe4=(df_train$Europe=="4")
df_train$Europe6=(df_train$Europe=="6")
df_train$Europe7=(df_train$Europe=="7")
df_train$Europe8=(df_train$Europe=="8")
df_train$Europe9=(df_train$Europe=="9")
df_train$Europe10=(df_train$Europe=="10")
df_train$Europe11=(df_train$Europe=="11")
df_train$political.knowledge2 =(df_train$political.knowledge=="2")
df_train$political.knowledge3 =(df_train$political.knowledge=="3")

df_test$Blair4=(df_test$Blair=="4")
df_test$Blair5=(df_test$Blair=="5")
df_test$Hague2=(df_test$Hague=="2")
df_test$Hague4=(df_test$Hague=="4")
df_test$Hague5=(df_test$Hague=="5")
df_test$Kennedy4=(df_test$Kennedy=="4")
df_test$Kennedy5=(df_test$Kennedy=="5")
df_test$Europe4=(df_test$Europe=="4")
df_test$Europe6=(df_test$Europe=="6")
df_test$Europe7=(df_test$Europe=="7")
df_test$Europe8=(df_test$Europe=="8")
df_test$Europe9=(df_test$Europe=="9")
df_test$Europe10=(df_test$Europe=="10")
df_test$Europe11=(df_test$Europe=="11")
df_test$political.knowledge2 =(df_test$political.knowledge=="2")
df_test$political.knowledge3 =(df_test$political.knowledge=="3")

glm.fit = glm(vote~age+Blair4+Blair5+Hague2+Hague4+Hague5+Kennedy4+Kennedy5
              +Europe4+Europe6+Europe7+Europe8+Europe9+Europe10+Europe11+
                political.knowledge2+political.knowledge3,data = df_train,
              family = binomial)
summary(glm.fit)

glm.pred = predict(glm.fit, df_test, type="response")
predictions = (glm.pred>.5) # try different cut-offs

# performance
table(predictions, df_test$vote)
df_test$vote_l=ifelse(df_test$vote=="1", FALSE,TRUE)
table(predictions, df_test$vote_l)

# Performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote_l))

#ROC
library(pROC)
glm.roc=roc(df_test$vote, glm.pred)
plot(glm.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(glm.roc, "best")

#########Upsample
glm.fit = glm(vote~.,data = train_upsampled,family = binomial)
summary(glm.fit)

#select
train_upsampled$economic.cond.household4=(train_upsampled$economic.cond.household=="4")
train_upsampled$Blair4=(train_upsampled$Blair=="4")
train_upsampled$Blair5=(train_upsampled$Blair=="5")
train_upsampled$Hague2=(train_upsampled$Hague=="2")
train_upsampled$Hague3=(train_upsampled$Hague=="3")
train_upsampled$Hague4=(train_upsampled$Hague=="4")
train_upsampled$Hague5=(train_upsampled$Hague=="5")
train_upsampled$Kennedy4=(train_upsampled$Kennedy=="4")
train_upsampled$Kennedy5=(train_upsampled$Kennedy=="5")
train_upsampled$Europe4=(train_upsampled$Europe=="4")
train_upsampled$Europe5=(train_upsampled$Europe=="5")
train_upsampled$Europe6=(train_upsampled$Europe=="6")
train_upsampled$Europe7=(train_upsampled$Europe=="7")
train_upsampled$Europe8=(train_upsampled$Europe=="8")
train_upsampled$Europe9=(train_upsampled$Europe=="9")
train_upsampled$Europe10=(train_upsampled$Europe=="10")
train_upsampled$Europe11=(train_upsampled$Europe=="11")
train_upsampled$political.knowledge2 =(train_upsampled$political.knowledge=="2")
train_upsampled$political.knowledge3 =(train_upsampled$political.knowledge=="3")

df_test$economic.cond.household4=(df_test$economic.cond.household=="4")
df_test$Blair4=(df_test$Blair=="4")
df_test$Blair5=(df_test$Blair=="5")
df_test$Hague2=(df_test$Hague=="2")
df_test$Hague3=(df_test$Hague=="3")
df_test$Hague4=(df_test$Hague=="4")
df_test$Hague5=(df_test$Hague=="5")
df_test$Kennedy4=(df_test$Kennedy=="4")
df_test$Kennedy5=(df_test$Kennedy=="5")
df_test$Europe4=(df_test$Europe=="4")
df_test$Europe5=(df_test$Europe=="5")
df_test$Europe6=(df_test$Europe=="6")
df_test$Europe7=(df_test$Europe=="7")
df_test$Europe8=(df_test$Europe=="8")
df_test$Europe9=(df_test$Europe=="9")
df_test$Europe10=(df_test$Europe=="10")
df_test$Europe11=(df_test$Europe=="11")
df_test$political.knowledge2 =(df_test$political.knowledge=="2")
df_test$political.knowledge3 =(df_test$political.knowledge=="3")

glm.fit = glm(vote~age+economic.cond.household4+Blair4+Blair5+Hague2
              +Hague3+Hague4+Hague5+Kennedy4+Kennedy5+Europe4+Europe5
              +Europe6+Europe7+Europe8+Europe9+Europe10+Europe11+
                political.knowledge2+political.knowledge3+gender,data
              = train_upsampled,family = binomial)
summary(glm.fit)

glm.pred=predict(glm.fit,df_test,type = "response")
predictions=(glm.pred<.5)

# performance
table(predictions, df_test$vote)
df_test$vote_l=ifelse(df_test$vote=="1", TRUE, FALSE)
table(predictions, df_test$vote_l)


# Performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote_l))

glm.roc=roc(df_test$vote, glm.pred)
plot(glm.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(glm.roc, "best")
```
```{r}
############## recursive partitioning tree 这里没有ROC
library(rpart)
df.rp = rpart(vote ~ ., data=df_train)
summary(df.rp)

# visualize the tree
plot(df.rp)
text(df.rp)
# use parameters to adjust the layout
plot(df.rp, uniform=TRUE, branch=0.1, margin=0.1)
text(df.rp, all=TRUE, use.n=TRUE)

# performance
###Sample
predictions = predict(df.rp, df_test, type="class")
table(predictions, df_test$vote)
# performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

rp.prob<-predict(df.rp, df_test, type="prob")
ROC<-roc(df_test$vote,rp.prob[,2])
plot(ROC, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

###Upsample
library(rpart)
df.rp = rpart(vote ~ ., data=train_upsampled)
summary(df.rp)

# visualize the tree
plot(df.rp)
text(df.rp)
# use parameters to adjust the layout
plot(df.rp, uniform=TRUE, branch=0.1, margin=0.1)
text(df.rp, all=TRUE, use.n=TRUE)

# performance
###Sample
predictions = predict(df.rp, df_test, type="class")
table(predictions, df_test$vote)
# performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

rp.prob<-predict(df.rp, df_test, type="prob")
rp.roc<-roc(df_test$vote,rp.prob[,2])
plot(rp.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
############ conditional inference tree
library(party)
df.ci = ctree(vote ~ ., data=df_train)
df.ci

# visualize the tree
plot(df.ci) # too big 
# export the tree graph
jpeg(file="c:/Users/75228/Desktop/BA/df.ci.jpg",width=2000,height=1000)
plot(df.ci, main="CI Tree") 
dev.off() # close the plot

# performance
predictions = predict(df.ci, df_test)
table(predictions, df_test$vote)
# performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

ci.prob<-predict(df.ci, df_test, type="node")
ci.roc<-roc(df_test$vote,ci.prob)
plot(ci.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

####Upsample
library(party)
df.ci = ctree(vote ~ ., data=train_upsampled)
df.ci

# visualize the tree
plot(df.ci) # too big 
# export the tree graph
jpeg(file="c:/Users/75228/Desktop/BA/df.ci.up.jpg",width=2000,height=1000)
plot(df.ci, main="CI Tree") 
dev.off() # close the plot

# performance
predictions = predict(df.ci, df_test)
table(predictions, df_test$vote)
# performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

ci.prob<-predict(df.ci, df_test, type="node")
ci.roc<-roc(df_test$vote,ci.prob)
plot(ci.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
################NaiveBayes## ROC
library(e1071)
df.NB = naiveBayes(df_train[,!names(df_train) %in% c("vote")], df_train$vote)
df.NB

# performance
predictions = predict(df.NB, df_test[,!names(df_test) %in% c("vote")])

table(predictions, df_test$vote)
library(caret)
confusionMatrix(table(predictions, df_test$vote))

nb.prob<-predict(df.NB, df_test, type="raw")
nb.roc<-roc(df_test$vote,nb.prob[,2])
plot(nb.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

##Upsample
df.NB = naiveBayes(train_upsampled[,!names(train_upsampled) %in% c("vote")], train_upsampled$vote)
df.NB

predictions = predict(df.NB, df_test[,!names(df_test) %in% c("vote")])

table(predictions,  df_test$vote)
library(caret)
confusionMatrix(table(predictions, df_test$vote))

nb.prob<-predict(df.NB, df_test, type="raw")
nb.roc<-roc(df_test$vote,nb.prob[,2])
plot(nb.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
############ random forest########no ROC
library(randomForest)
df.rf = randomForest(vote ~ ., data=df_train, importance=T)
df.rf
importance(df.rf)

# performance
predictions = predict(df.rf, df_test)
table(predictions, df_test$vote)
# Performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

rf.prob<-predict(df.rf, df_test, type="prob")
rf.roc<-roc(df_test$vote,rf.prob[,2])
plot(rf.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

###Upsample
df.rf = randomForest(vote ~ ., data=train_upsampled, importance=T)
df.rf
importance(df.rf)

# performance
predictions = predict(df.rf, df_test)
table(predictions, df_test$vote)
# Performance: Confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

rf.prob<-predict(df.rf, df_test, type="prob")
rf.roc<-roc(df_test$vote,rf.prob[,2])
plot(rf.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
####################linear discriminant analysis(LDA)
library (MASS) 
lda.fit=lda(vote~.,data=df_train)
lda.fit
lda.pred=predict(lda.fit,df_test,type="response")

# performance
lda.class=lda.pred$class
table(lda.class, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(lda.class, df_test$vote))

lda.prob<-predict(lda.fit, df_test)
lda.roc<-roc(df_test$vote,lda.prob$posterior[,2])
plot(lda.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

#Upsample####
lda.fit=lda(vote~.,data=train_upsampled)
lda.fit
lda.pred=predict(lda.fit,df_test,type="response")

# performance
lda.class=lda.pred$class
table(lda.class, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(lda.class, df_test$vote))

lda.prob<-predict(lda.fit, df_test)
lda.roc<-roc(df_test$vote,lda.prob$posterior[,2])
plot(lda.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
################SVM#############
library(e1071)
df.SVM = svm(vote~., data=df_train, kernel="radial", cost=1, gamma=1/ncol(df_train),probability = TRUE)
summary(df.SVM)

# performance
predictions = predict(df.SVM, df_test[,!names(df_test) %in% c("vote")])
table(predictions, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

svm.pred<-predict(df.SVM, df_test,probability=TRUE,decision.values=TRUE)
svm.prob<-attr(svm.pred,'probabilities')[,2]
svm.roc<-roc(df_test$vote,svm.prob)
plot(svm.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")

####Upsample
library(e1071)
df.SVM = svm(vote~., data=train_upsampled, kernel="radial", 
             cost=1, gamma=1/ncol(train_upsampled),probability=TRUE)
summary(df.SVM)

# performance
predictions = predict(df.SVM, df_test[,!names(df_test) %in% c("vote")])
table(predictions, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

svm.pred<-predict(df.SVM, df_test,probability=TRUE,decision.values=TRUE)
svm.prob<-attr(svm.pred,'probabilities')[,2]
svm.roc<-roc(df_test$vote,svm.prob)
plot(svm.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE, print.thres=TRUE,auc.polygon.col="skyblue")
```
```{r}
# GBM#################################################################
###change "vote" data type to dummy/numeric
# GBM
library(gbm)

head(df_train)
df_train=df_train[,!names(df_train) %in% c("1", "2")]
df_test=df_test[,!names(df_test) %in% c("1", "2")]

df_train$vote = ifelse(df_train$vote=="1", 1, 0)
df_test$vote = ifelse(df_test$vote=="1", 1, 0)

df.GBM=gbm(vote~., distribution="bernoulli", data=df_train, n.trees=1000, 
              interaction.depth=7, shrinkage=0.01, cv.folds = 3)
summary(df.GBM)

# prediction
pred = predict(df.GBM, df_test, n.trees=1000)

# finding the best cut-off 
# install.packages("pROC")
library(pROC)
df.roc=roc(df_test$vote, pred)
plot(df.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(df.roc, "best")

# obtaining the classification table
predictions = ifelse(pred>coords(df.roc, "best") ["threshold"], 1, 0)
table(predictions, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))

#Upsample
head(train_upsampled)
train_upsampled=train_upsampled[,!names(train_upsampled) %in% c("1", "2")]
df_test=df_test[,!names(df_test) %in% c("1", "2")]

train_upsampled$vote = ifelse(train_upsampled$vote=="1", 1, 0)
df_test$vote = ifelse(df_test$vote=="1", 1, 0)

str(df)
set.seed(1)
df.GBM=gbm(vote~., distribution="bernoulli", data=train_upsampled, n.trees=1000, 
           interaction.depth=7, shrinkage=0.01, cv.folds = 3)
summary(df.GBM)

# prediction
pred = predict(df.GBM, df_test, n.trees=1000)

# finding the best cut-off 
# install.packages("pROC")
library(pROC)
df.roc=roc(df_test$vote, pred)
plot(df.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(df.roc, "best")

# obtaining the classification table
predictions = ifelse(pred>coords(df.roc, "best") ["threshold"], 1, 0)
table(predictions, df_test$vote)
# performance: confusion matrix
library(caret)
confusionMatrix(table(predictions, df_test$vote))
```
```{r}
###此处所有变量为numeric
##########KNN
df=read.csv('BEPS.csv')
attach(df)
df=df[,-1]
df$age<-as.numeric(df$age)
df$economic.cond.national<-as.numeric(df$economic.cond.national)
df$economic.cond.household<-as.numeric(df$economic.cond.household)
df$Blair<-as.numeric(df$Blair)
df$Hague<-as.numeric(df$Hague)
df$Kennedy<-as.numeric(df$Kennedy)
df$Europe<-as.numeric(df$Europe)
df$gender<-as.numeric(df$gender)
levels(df$vote)[levels(df$vote)%in% c("Labour","Liberal Democrat")]<-"0"
levels(df$vote)[levels(df$vote)%in% c("Conservative")]<-"1"
df$vote<-as.numeric(df$vote)
set.seed(1)
#  70% observations work as training data
subset = sample(1:nrow(df), 1068) 
df_train = df[subset,]
# The rest 30% are test data
df_test = df[-subset,]
###KNN
library(MLmetrics)
Xtrain=df_train[,-1]
Xtest=df_test[,-1]
library(class)
k_vector = 1:10
f1 = rep(0,length(k_vector))
for (k in k_vector){
  set.seed(1)
  prediction = knn(Xtrain,Xtest,df_train$vote,k)
  f1[k]=F1_Score(df_test$vote,prediction,positive = '1')
}
best_k<-which.max(f1)
max(f1,na.rm=TRUE)
KNN.predictions = knn(df_train[,!names(df_train) %in% c("vote")],
                df_test[,!names(df_test) %in% c("vote")],df_train$vote,k=best_k,prob=TRUE)
prob <- attr(KNN.predictions, "prob")
# performance
table(KNN.predictions, df_test$vote)
# Performance: Confusion matrix
library(caret)
confusionMatrix(table(KNN.predictions, df_test$vote))

#ROC
knn.roc=roc(df_test$vote, prob)
plot(knn.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(knn.roc, "best")

####upsampling for knn
df=read.csv('BEPS.csv')
attach(df)
df=df[,-1]
str(df)
head(df)

levels(df$vote)[levels(df$vote)%in% c("Labour","Liberal Democrat")]<-"0"
levels(df$vote)[levels(df$vote)%in% c("Conservative")]<-"1"
dim(df)
sum(df_test$vote=="1")
#data preprocessing

sum(is.na(df))#

df$age<-as.numeric(df$age)
df$economic.cond.national<-as.numeric(df$economic.cond.national)
df$economic.cond.household<-as.numeric(df$economic.cond.household)
df$Blair<-as.numeric(df$Blair)
df$Hague<-as.numeric(df$Hague)
df$Kennedy<-as.numeric(df$Kennedy)
df$Europe<-as.numeric(df$Europe)
df$gender<-as.numeric(df$gender)
df$vote<-as.numeric(df$vote)

#Train Test Split
# Set random seed
set.seed(1)
#  70% observations work as training data
subset = sample(1:nrow(df), 1068) 
df_train = df[subset,]
# The rest 30% are test data
df_test = df[-subset,]

train_majority = df_train[which(df_train$vote==2),]
train_minority = df_train[which(df_train$vote==1),]
subset2= sample(1:nrow(train_minority),nrow(train_majority),replace = TRUE)
train_minority_upsampled = train_minority[subset2,] # reproducible results
train_upsampled = rbind(train_majority, train_minority_upsampled)
dim(train_upsampled )
table(train_upsampled$vote) #check whether we get balanced class label

Xtrain=train_upsampled[,-1]
Xtest=df_test[,-1]
library(class)
k_vector = 1:10
f1 = rep(0,length(k_vector))
for (k in k_vector){
  set.seed(1)
  prediction = knn(Xtrain,Xtest,train_upsampled$vote,k)
  f1[k]=F1_Score(df_test$vote,prediction,positive = '1')
}
best_k<-which.max(f1)
max(f1,na.rm=TRUE)

KNN.predictions = knn(train_upsampled[,!names(train_upsampled) %in% c("vote")],
                df_test[,!names(df_test) %in% c("vote")],train_upsampled$vote,k=best_k,prob=TRUE)
prob <- attr(KNN.predictions, "prob")

# performance
table(KNN.predictions, df_test$vote)
# Performance: Confusion matrix
library(caret)
confusionMatrix(table(KNN.predictions, df_test$vote))

#ROC
knn.roc=roc(df_test$vote, prob)
plot(knn.roc,print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2) ,
     max.auc.polygon=TRUE,print.thres=TRUE,auc.polygon.col="skyblue")
coords(knn.roc, "best")
